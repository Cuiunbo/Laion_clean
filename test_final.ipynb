{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.CleanData import Operate, filter_language\n",
    "from utils.FilterData import FilterPara\n",
    "\n",
    "import os\n",
    "import time\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "import clip\n",
    "import dask\n",
    "import faiss\n",
    "import dask.array as da\n",
    "\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1024)\n",
    "\n",
    "\n",
    "# min_ratio = 0.9\n",
    "# max_ratio = 1.1\n",
    "\n",
    "# min_width, max_width = 128, 1024\n",
    "# min_height, max_height = 128, 1024\n",
    "\n",
    "# min_length, max_length = 16,1024\n",
    "\n",
    "# desired_rows_per_partition = 6000\n",
    "# batch_size = 3200\n",
    "\n",
    "# # faiss\n",
    "# faiss_k = 100  # 只搜索最近的100个邻居\n",
    "# faiss_threshold = 0.98 # image feature cos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df, showlen = 5):\n",
    "    print(f\"lenth : {len(df)}\")\n",
    "    print(f\"{df.head(showlen)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = '/mnt/alluxio/alluxio-fuse/user/tc_agi/klara/datasets/laion2b_en/laion2b_en_20230417112304'\n",
    "# directory = '/mnt/alluxio/alluxio-fuse/user/tc_agi/klara/datasets/laion2b_multi_chinese_subset/laion2b_multi_chinese_subset'\n",
    "files = glob(os.path.join(directory, '*.parquet'))\n",
    "files = [f for f in files if not os.path.basename(f).startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files = files[100:110]\n",
    "sample_df = dd.read_parquet(sample_files)\n",
    "\n",
    "min_ratio = 0.9\n",
    "max_ratio = 1.1\n",
    "sample_df['RATIO'] = sample_df['WIDTH'] / sample_df['HEIGHT']\n",
    "\n",
    "sample_df = sample_df[(sample_df['RATIO'] >= min_ratio) & (sample_df['RATIO'] <= max_ratio)]\n",
    "sample_df = sample_df.drop('RATIO', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_width, max_width = 128, 1024\n",
    "min_height, max_height = 128, 1024\n",
    "\n",
    "sample_df = sample_df[(sample_df['WIDTH'] < max_width) & (sample_df['HEIGHT'] < max_height)]\n",
    "sample_df = sample_df[(sample_df['WIDTH'] > min_width) & (sample_df['HEIGHT'] > min_height)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length, max_length = 16,1024\n",
    "sample_df= sample_df[(sample_df['TEXT'].str.len() >= min_length) & (sample_df['TEXT'].str.len() <= max_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sample_df\n",
    "df['TEXT'] = df['TEXT'].map(Operate, meta=('TEXT', 'object'))\n",
    "df = df[df['TEXT'].apply(filter_language, meta=('TEXT', 'bool'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "df_clip = df.compute()\n",
    "\n",
    "desired_rows_per_partition = 6000\n",
    "\n",
    "npartitions = len(df_clip) // desired_rows_per_partition\n",
    "df_clip = dd.from_pandas(df_clip, npartitions=npartitions)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"/mnt/data/user/tc_agi/multi_modal/checkpoints/clip/ViT-B-16.pt\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(row):\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(row['BUFFER']))\n",
    "        tensor = preprocess(img).to(device)\n",
    "        return tensor, row\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None, None\n",
    "\n",
    "gpu_lock = threading.Lock()\n",
    "\n",
    "def compute_embeddings(partition, batch_size=3200):\n",
    "    with gpu_lock:\n",
    "\n",
    "        all_embeddings = []\n",
    "        valid_rows = []  \n",
    "\n",
    "        for start in range(0, len(partition), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch = partition.iloc[start:end]\n",
    "            print(len(batch))\n",
    "            processed_tensors = []\n",
    "            for _, row in batch.iterrows():\n",
    "                tensor, valid_row = process_image(row)\n",
    "                if tensor is not None:\n",
    "                    processed_tensors.append(tensor)\n",
    "                    valid_rows.append(valid_row)\n",
    "\n",
    "            tensor_stack = torch.stack(processed_tensors)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                embedding = model.encode_image(tensor_stack)\n",
    "            all_embeddings.extend(embedding.cpu().numpy())\n",
    "\n",
    "        valid_partition = pd.concat(valid_rows, axis=1).transpose()\n",
    "        valid_partition['CLIP_Features'] = all_embeddings\n",
    "\n",
    "        return valid_partition\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 3200\n",
    "df_clip = df_clip.map_partitions(compute_embeddings, batch_size=batch_size, meta=df_clip._meta.assign(CLIP_Features='f8'))\n",
    "result = df_clip.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = result\n",
    "df_pandas = df_pandas.set_index('SAMPLE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_k = 100  # 只搜索最近的100个邻居\n",
    "faiss_threshold = 0.98 # image feature cos \n",
    "\n",
    "\n",
    "embeddings_matrix = np.vstack(df_pandas['CLIP_Features'].to_list()).astype('float32')  \n",
    "faiss.normalize_L2(embeddings_matrix)\n",
    "\n",
    "nlist = 100\n",
    "quantizer = faiss.IndexFlatL2(embeddings_matrix.shape[1])\n",
    "index_cpu = faiss.IndexIVFFlat(quantizer, embeddings_matrix.shape[1], nlist, faiss.METRIC_L2)\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index_cpu)\n",
    "\n",
    "assert not index.is_trained\n",
    "index.train(embeddings_matrix)\n",
    "assert index.is_trained\n",
    "\n",
    "index.add(embeddings_matrix)\n",
    "\n",
    "k = faiss_k  # 只搜索最近的100个邻居\n",
    "D, I = index.search(embeddings_matrix, k)\n",
    "\n",
    "threshold = faiss_threshold\n",
    "similar_pairs = []\n",
    "\n",
    "for i in range(I.shape[0]):\n",
    "    similarities = 1 - D[i] / 2  \n",
    "    filtered_sample_ids = df_pandas.index[I[i][(similarities > threshold) & (I[i] != i)]].tolist()\n",
    "    if filtered_sample_ids:\n",
    "        similar_pairs.append((df_pandas.index[i], filtered_sample_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = result\n",
    "df1 = df1.set_index('SAMPLE_ID')\n",
    "df1['CLIP_Features'] = df1['CLIP_Features'].apply(lambda x: np.array(x).astype('float32') if isinstance(x, (list, np.ndarray)) else x)\n",
    "\n",
    "df_to_save = df1[['CLIP_Features']]\n",
    "\n",
    "df_to_save.to_parquet('saved_data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def retrieve_and_show_images(sample_id1, similar_sample_ids, dataframe, max_similar=5):\n",
    "    \n",
    "    similar_sample_ids = [sid for sid in similar_sample_ids if sid != sample_id1]\n",
    "    \n",
    "    image1 = Image.open(io.BytesIO(dataframe.loc[sample_id1]['BUFFER']))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1, min(len(similar_sample_ids), max_similar) + 1, 1)\n",
    "    plt.imshow(image1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    for j, sample_id2 in enumerate(similar_sample_ids[:max_similar], start=2): \n",
    "        image2 = Image.open(io.BytesIO(dataframe.loc[sample_id2]['BUFFER']))\n",
    "        plt.subplot(1, min(len(similar_sample_ids), max_similar) + 1, j)\n",
    "        plt.imshow(image2)\n",
    "        plt.axis('off') # 移除坐标轴\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for idx, (sample_id1, similar_sample_ids) in enumerate(similar_pairs):\n",
    "    if idx >= 40: \n",
    "        break\n",
    "    retrieve_and_show_images(sample_id1, similar_sample_ids, df_pandas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def is_image_valid(image_data):\n",
    "    try:\n",
    "        with Image.open(io.BytesIO(image_data)) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df['is_valid'] = df['BUFFER'].map(is_image_valid, meta=('BUFFER', 'bool'))\n",
    "\n",
    "df_remove = df[df['is_valid'] == False]\n",
    "df = df[df['is_valid'] == True]\n",
    "\n",
    "df = df.drop('is_valid', axis=1)\n",
    "\n",
    "computed_df = df.compute(scheduler='processes')  \n",
    "computed_df_remove = df_remove.compute(scheduler='processes')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>WIDTH</th>\n",
       "      <th>LICENSE</th>\n",
       "      <th>NSFW</th>\n",
       "      <th>similarity</th>\n",
       "      <th>BUFFER</th>\n",
       "      <th>IMG_TYPE</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7903</th>\n",
       "      <td>4.186990e+12</td>\n",
       "      <td>https://cdn.shopify.com/s/files/1/0016/4390/51...</td>\n",
       "      <td>Gold Pigger Ring Women</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "      <td>?</td>\n",
       "      <td>UNLIKELY</td>\n",
       "      <td>0.314278</td>\n",
       "      <td>b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...</td>\n",
       "      <td>png</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SAMPLE_ID                                                URL   \n",
       "7903  4.186990e+12  https://cdn.shopify.com/s/files/1/0016/4390/51...  \\\n",
       "\n",
       "                        TEXT  HEIGHT  WIDTH LICENSE      NSFW  similarity   \n",
       "7903  Gold Pigger Ring Women     960    960       ?  UNLIKELY    0.314278  \\\n",
       "\n",
       "                                                 BUFFER IMG_TYPE  is_valid  \n",
       "7903  b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...      png     False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_df_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10614\n",
      "10615\n"
     ]
    }
   ],
   "source": [
    "print(len(result))\n",
    "print(len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeeves/.local/lib/python3.10/site-packages/dask/dataframe/core.py:4421: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('BUFFER', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized method took: 38.581528186798096 seconds\n",
      "Loop-based method took: 42.90600252151489 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def vectorized_method(partition):\n",
    "    partition['Image'] = partition['BUFFER'].apply(safe_open)\n",
    "    partition.compute()\n",
    "\n",
    "def loop_based_method(partition):\n",
    "    images = []\n",
    "    for idx, row in partition.iterrows():\n",
    "        img = safe_open(row['BUFFER'])\n",
    "\n",
    "start_time = time.time()\n",
    "vectorized_method(filtered_df) \n",
    "vectorized_duration = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "loop_based_method(filtered_df)  \n",
    "loop_based_duration = time.time() - start_time\n",
    "\n",
    "print(f\"Vectorized method took: {vectorized_duration} seconds\")\n",
    "print(f\"Loop-based method took: {loop_based_duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# def parallel_preprocess(buffer):\n",
    "#     try:\n",
    "#         img = Image.open(io.BytesIO(buffer))\n",
    "#         return preprocess(img)\n",
    "#     except Exception as e:\n",
    "#         return None\n",
    "\n",
    "# def compute_embeddings(partition, batch_size=4000):\n",
    "#     all_embeddings = []\n",
    "\n",
    "#     n_processes = os.cpu_count()\n",
    "\n",
    "#     with Pool(n_processes) as pool:\n",
    "#         partition['Processed_Image'] = list(pool.map(parallel_preprocess, partition['BUFFER'].tolist()))\n",
    "\n",
    "#     partition['Is_Exception'] = partition['Processed_Image'].isnull()\n",
    "#     valid_partition = partition[~partition['Is_Exception']]\n",
    "    \n",
    "#     tensors = [tensor for tensor in valid_partition['Processed_Image'].tolist() if tensor is not None]\n",
    "    \n",
    "#     for start in range(0, len(tensors), batch_size):\n",
    "#         end = start + batch_size\n",
    "#         batch_tensors = tensors[start:end]\n",
    "\n",
    "#         tensor_stack = torch.stack(batch_tensors).to(device)\n",
    "#         with torch.no_grad():\n",
    "#             embedding = model.encode_image(tensor_stack)\n",
    "#         all_embeddings.extend(embedding.cpu().numpy())\n",
    "\n",
    "#     valid_partition['CLIP_Features'] = all_embeddings\n",
    "\n",
    "#     return valid_partition\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
